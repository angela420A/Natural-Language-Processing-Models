{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccbea25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f26b0",
   "metadata": {},
   "source": [
    "# Corpus vocabulary\n",
    "----\n",
    "\n",
    "A text corpus's unique words make up its **vocabulary**. This vocabulary forms the foundation for all NLP text processing. \n",
    "\n",
    "- **Character-based**\n",
    "- **Word-based**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a3f36",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "-----\n",
    "The most important part of the vocabulary is that it allows us to represent each piece of text by the specific words that appear in it.\n",
    "\n",
    "Rather than being represented as one long string, a piece of text can be represented as a vector/list of its vocabulary words. This process is known as tokenization, where each individual vocabulary word in a piece of text is a token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf33985c",
   "metadata": {},
   "source": [
    "## Tokenizer Object\n",
    "\n",
    "`fit_on_texts` updates the internal vocabulary of the `Tokenizer` based on the word frequency in the input texts.\n",
    "\n",
    "- It creates a word index: a dictionary mapping words to integers.\n",
    "- It does **not** transform the texts into sequences — that’s done by `texts_to_sequences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f54145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning': 1, 'i': 2, 'love': 3, 'deep': 4, 'machine': 5, 'is': 6, 'amazing': 7}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "# texts\n",
    "texts = [\n",
    "    \"I love machine learning\",\n",
    "    \"Deep learning is amazing\",\n",
    "    \"I love deep learning\"\n",
    "]\n",
    "\n",
    "# Build the word index based on the frequency of words in the input texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Print the dictionary that maps words to their integer index\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a372667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 5, 1], [4, 1, 6, 7], [2, 3, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Convert each text in the list into a sequence of integers based on the word index\n",
    "print(tokenizer.texts_to_sequences(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338f33b",
   "metadata": {},
   "source": [
    "## Tokenizer parameters\n",
    "The Tokenizer object can be initialized with a number of optional parameters. By default, the Tokenizer filters out any punctuation and white space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c9678f",
   "metadata": {},
   "source": [
    "### Out-Of-Vocabulary (OOV)\n",
    "When a new text contains words not in the corpus vocabulary, those words are known as out-of-vocabulary (OOV).\n",
    "\n",
    "The `texts_to_sequences` automatically filters out all OOV words. However, if we want to specify each OOV word with a special vocabulary token (e.g. `OOV`), we can initialize the Tokenizer with the `oov_token` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734c634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Tokenizer and specify a special token for out-of-vocabulary (OOV) words\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='OOV')\n",
    "\n",
    "# texts\n",
    "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
    "tokenizer.fit_on_texts(text_corpus)\n",
    "\n",
    "# Convert a new sentence into a sequence of integers\n",
    "# Words not seen in the corpus ('bacon', 'orange') are mapped to the OOV token (which is `1`)\n",
    "print(tokenizer.texts_to_sequences(['bob ate bacon orange']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf06fb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OOV': 1, 'ate': 2, 'apples': 3, 'bob': 4, 'and': 5, 'pears': 6, 'fred': 7}\n"
     ]
    }
   ],
   "source": [
    "# Show the word index dictionary\n",
    "# Includes all words from the corpus and the 'OOV' token\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4294a4e",
   "metadata": {},
   "source": [
    "### num-words\n",
    "The `num_words` parameter lets us specify the maximum number of vocabulary words to use. For example, if we set `num_words=100` when initializing the `Tokenizer`, it will only use the 100 most frequent words in the vocabulary and filter out the remaining vocabulary words.\n",
    "\n",
    "*This can be useful when the text corpus is large and you need to limit the vocabulary size to increase training speed or prevent overfitting on infrequent words.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a35a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer that only keeps the top (num_words - 1) = 1 most frequent word\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=2)\n",
    "\n",
    "# Fit tokenizer on the text corpus to build the word index\n",
    "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
    "tokenizer.fit_on_texts(text_corpus)\n",
    "\n",
    "# Convert new sentence to sequences\n",
    "# Only the word 'ate' is kept because it's the most frequent (index 1)\n",
    "# All other words are filtered out since their indices are >= 2\n",
    "print(tokenizer.texts_to_sequences(['bob ate pears apples']))  # Output: [[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e869d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
